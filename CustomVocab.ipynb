{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load custom vocab for odd data types like music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer('vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method tokenize in module transformers.tokenization_utils:\n",
      "\n",
      "tokenize(text: str, **kwargs) -> List[str] method of transformers.tokenization_bert.BertTokenizer instance\n",
      "    Converts a string in a sequence of tokens, using the tokenizer.\n",
      "    \n",
      "    Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies (BPE/SentencePieces/WordPieces).\n",
      "    Takes care of added tokens.\n",
      "    \n",
      "    Args:\n",
      "        text (:obj:`str`):\n",
      "            The sequence to be encoded.\n",
      "        **kwargs (additional keyword arguments):\n",
      "            Passed along to the model-specific ``prepare_for_tokenization`` preprocessing method.\n",
      "    \n",
      "    Returns:\n",
      "        :obj:`List[str]`: The list of tokens.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = BertTokenizer('vocab.txt').get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 388, 1, 383, 18, 484, 1, 472, 39]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = 'WT_16 P1_NOTEON_50 WT_2 P1_NOTEON_45 WT_19 P2_NOTEON_69 WT_2 P2_NOTEON_57 WT_40'\n",
    "[vocab[w] for w in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on FunnelTokenizer in module transformers.tokenization_funnel object:\n",
      "\n",
      "class FunnelTokenizer(transformers.tokenization_bert.BertTokenizer)\n",
      " |  FunnelTokenizer(vocab_file, do_lower_case=True, do_basic_tokenize=True, never_split=None, unk_token='<unk>', sep_token='<sep>', pad_token='<pad>', cls_token='<cls>', mask_token='<mask>', bos_token='<s>', eos_token='</s>', tokenize_chinese_chars=True, strip_accents=None, **kwargs)\n",
      " |  \n",
      " |  Construct a Funnel Transformer tokenizer.\n",
      " |  \n",
      " |  :class:`~transformers.FunnelTokenizer` is identical to :class:`~transformers.BertTokenizer` and runs end-to-end\n",
      " |  tokenization: punctuation splitting and wordpiece.\n",
      " |  \n",
      " |  Refer to superclass :class:`~transformers.BertTokenizer` for usage examples and documentation concerning\n",
      " |  parameters.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FunnelTokenizer\n",
      " |      transformers.tokenization_bert.BertTokenizer\n",
      " |      transformers.tokenization_utils.PreTrainedTokenizer\n",
      " |      transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
      " |      transformers.tokenization_utils_base.SpecialTokensMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, vocab_file, do_lower_case=True, do_basic_tokenize=True, never_split=None, unk_token='<unk>', sep_token='<sep>', pad_token='<pad>', cls_token='<cls>', mask_token='<mask>', bos_token='<s>', eos_token='</s>', tokenize_chinese_chars=True, strip_accents=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  create_token_type_ids_from_sequences(self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) -> List[int]\n",
      " |      Create a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
      " |      A Funnel Transformer sequence pair mask has the following format:\n",
      " |      \n",
      " |      ::\n",
      " |      \n",
      " |          2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      " |          | first sequence    | second sequence |\n",
      " |      \n",
      " |      If :obj:`token_ids_1` is :obj:`None`, this method only returns the first portion of the mask (0s).\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`):\n",
      " |              List of IDs.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`):\n",
      " |              Optional second list of IDs for sequence pairs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`: List of `token type IDs <../glossary.html#token-type-ids>`_ according to the given\n",
      " |          sequence(s).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'cls_token_type_id': <class 'int'>}\n",
      " |  \n",
      " |  cls_token_type_id = 2\n",
      " |  \n",
      " |  max_model_input_sizes = {'funnel-transformer/intermediate': 512, 'funn...\n",
      " |  \n",
      " |  pretrained_init_configuration = {'funnel-transformer/intermediate': {'...\n",
      " |  \n",
      " |  pretrained_vocab_files_map = {'vocab_file': {'funnel-transformer/inter...\n",
      " |  \n",
      " |  vocab_files_names = {'vocab_file': 'vocab.txt'}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_bert.BertTokenizer:\n",
      " |  \n",
      " |  build_inputs_with_special_tokens(self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) -> List[int]\n",
      " |      Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
      " |      by concatenating and adding special tokens.\n",
      " |      A BERT sequence has the following format:\n",
      " |      \n",
      " |      - single sequence: ``[CLS] X [SEP]``\n",
      " |      - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`):\n",
      " |              List of IDs to which the special tokens will be added.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`):\n",
      " |              Optional second list of IDs for sequence pairs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`: List of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
      " |  \n",
      " |  convert_tokens_to_string(self, tokens)\n",
      " |      Converts a sequence of tokens (string) in a single string.\n",
      " |  \n",
      " |  get_special_tokens_mask(self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None, already_has_special_tokens: bool = False) -> List[int]\n",
      " |      Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n",
      " |      special tokens using the tokenizer ``prepare_for_model`` method.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`):\n",
      " |              List of IDs.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`):\n",
      " |              Optional second list of IDs for sequence pairs.\n",
      " |          already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the token list is already formatted with special tokens for the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
      " |  \n",
      " |  get_vocab(self)\n",
      " |      Returns the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      :obj:`tokenizer.get_vocab()[token]` is equivalent to :obj:`tokenizer.convert_tokens_to_ids(token)` when\n",
      " |      :obj:`token` is in the vocab.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Dict[str, int]`: The vocabulary.\n",
      " |  \n",
      " |  save_vocabulary(self, vocab_path)\n",
      " |      Save the vocabulary (copy original file) and special tokens file to a directory.\n",
      " |      \n",
      " |      Args:\n",
      " |          vocab_path (:obj:`str`):\n",
      " |              The directory in which to save the vocabulary.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple(str)`: Paths to the files saved.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_bert.BertTokenizer:\n",
      " |  \n",
      " |  vocab_size\n",
      " |      :obj:`int`: Size of the base vocabulary (without the added tokens).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Size of the full vocabulary with the added tokens.\n",
      " |  \n",
      " |  convert_ids_to_tokens(self, ids: Union[int, List[int]], skip_special_tokens: bool = False) -> Union[str, List[str]]\n",
      " |      Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary\n",
      " |      and added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`int` or :obj:`List[int]`):\n",
      " |              The token id (or token ids) to convert to tokens.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str` or :obj:`List[str]`: The decoded token(s).\n",
      " |  \n",
      " |  convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]\n",
      " |      Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the\n",
      " |      vocabulary.\n",
      " |      \n",
      " |      Args:\n",
      " |          token (:obj:`str` or :obj:`List[str]`): One or several token(s) to convert to token id(s).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int` or :obj:`List[int]`: The token id or list of token ids.\n",
      " |  \n",
      " |  decode(self, token_ids: List[int], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = True) -> str\n",
      " |      Converts a sequence of ids in a string, using the tokenizer and vocabulary\n",
      " |      with options to remove special tokens and clean up tokenization spaces.\n",
      " |      \n",
      " |      Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids (:obj:`List[int]`):\n",
      " |              List of tokenized input ids. Can be obtained using the ``__call__`` method.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to clean up the tokenization spaces.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The decoded sentence.\n",
      " |  \n",
      " |  get_added_vocab(self) -> Dict[str, int]\n",
      " |      Returns the added tokens in the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Dict[str, int]`: The added tokens.\n",
      " |  \n",
      " |  num_special_tokens_to_add(self, pair: bool = False) -> int\n",
      " |      Returns the number of added tokens when encoding a sequence with special tokens.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not\n",
      " |          put this inside your training loop.\n",
      " |      \n",
      " |      Args:\n",
      " |          pair (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether the number of added tokens should be computed in the case of a sequence pair or a single\n",
      " |              sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of special tokens added to sequences.\n",
      " |  \n",
      " |  prepare_for_tokenization(self, text: str, is_split_into_words: bool = False, **kwargs) -> Tuple[str, Dict[str, Any]]\n",
      " |      Performs any necessary transformations before tokenization.\n",
      " |      \n",
      " |      This method should pop the arguments from kwargs and return the remaining :obj:`kwargs` as well.\n",
      " |      We test the :obj:`kwargs` at the end of the encoding process to be sure all the arguments have been used.\n",
      " |      \n",
      " |      Args:\n",
      " |          test (:obj:`str`):\n",
      " |              The text to prepare.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the text has been pretokenized.\n",
      " |          kwargs:\n",
      " |              Keyword arguments to use for the tokenization.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple[str, Dict[str, Any]]`: The prepared text and the unused kwargs.\n",
      " |  \n",
      " |  prepare_seq2seq_batch(self, src_texts: List[str], tgt_texts: Union[List[str], NoneType] = None, max_length: Union[int, NoneType] = None, max_target_length: Union[int, NoneType] = None, padding: str = 'longest', return_tensors: str = 'None', truncation=True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepare a batch that can be passed directly to an instance of :class:`~transformers.AutoModelForSeq2SeqLM`.\n",
      " |      \n",
      " |      Args:\n",
      " |          src_texts: (:obj:`List[str]`):\n",
      " |              List of documents to summarize or source language texts.\n",
      " |          tgt_texts: (:obj:`List[str]`, `optional`):\n",
      " |              List of summaries or target language texts.\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length for encoder inputs (documents to summarize or source language texts).\n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          max_target_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length of decoder inputs (target language texts or summaries).\n",
      " |              If left unset or set to :obj:`None`, this will use the max_length value.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`, defaults to \"pt\"):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`True`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          **kwargs:\n",
      " |              Additional keyword arguments passed along to :obj:`self.__call__`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to the encoder.\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.\n",
      " |          - **labels** -- List of token ids for tgt_texts\n",
      " |      \n",
      " |          The full set of keys ``[input_ids, attention_mask, labels]``,\n",
      " |          will only be returned if tgt_texts is passed. Otherwise, input_ids, attention_mask will be the only keys.\n",
      " |  \n",
      " |  tokenize(self, text: str, **kwargs) -> List[str]\n",
      " |      Converts a string in a sequence of tokens, using the tokenizer.\n",
      " |      \n",
      " |      Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies (BPE/SentencePieces/WordPieces).\n",
      " |      Takes care of added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`):\n",
      " |              The sequence to be encoded.\n",
      " |          **kwargs (additional keyword arguments):\n",
      " |              Passed along to the model-specific ``prepare_for_tokenization`` preprocessing method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[str]`: The list of tokens.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  is_fast\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, List[str], List[List[str]]], text_pair: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      " |      sequences.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):\n",
      " |              The sequence or batch of sequences to be encoded.\n",
      " |              Each sequence can be a string or a list of strings (pretokenized string).\n",
      " |              If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):\n",
      " |              The sequence or batch of sequences to be encoded.\n",
      " |              Each sequence can be a string or a list of strings (pretokenized string).\n",
      " |              If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
      " |              will skip the pre-tokenization step. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Wheter or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print informations and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 0 specifying added special tokens and 1 specifying\n",
      " |            regual sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  batch_decode(self, sequences: List[List[int]], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = True) -> List[str]\n",
      " |      Convert a list of lists of token ids into a list of strings by calling decode.\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences (:obj:`List[List[int]]`):\n",
      " |              List of tokenized input ids. Can be obtained using the ``__call__`` method.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to clean up the tokenization spaces.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[str]`: The list of decoded sentences.\n",
      " |  \n",
      " |  batch_encode_plus(self, batch_text_or_text_pairs: Union[List[str], List[Tuple[str, str]], List[List[str]], List[Tuple[List[str], List[str]]], List[List[int]], List[Tuple[List[int], List[int]]]], add_special_tokens: bool = True, padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This method is deprecated, ``__call__`` should be used instead.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_text_or_text_pairs (:obj:`List[str]`, :obj:`List[Tuple[str, str]]`, :obj:`List[List[str]]`, :obj:`List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also :obj:`List[List[int]]`, :obj:`List[Tuple[List[int], List[int]]]`):\n",
      " |              Batch of sequences or pair of sequences to be encoded.\n",
      " |              This can be a list of string/string-sequences/int-sequences or a list of pair of\n",
      " |              string/string-sequences/int-sequence (see details in ``encode_plus``).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
      " |              will skip the pre-tokenization step. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Wheter or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print informations and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 0 specifying added special tokens and 1 specifying\n",
      " |            regual sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  encode(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, **kwargs) -> List[int]\n",
      " |      Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
      " |      \n",
      " |      Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      " |              ``convert_tokens_to_ids`` method).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n",
      " |              string using the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      " |              ``convert_tokens_to_ids`` method).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
      " |              will skip the pre-tokenization step. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          **kwargs: Passed along to the `.tokenize()` method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`, :obj:`torch.Tensor`, :obj:`tf.Tensor` or :obj:`np.ndarray`:\n",
      " |          The tokenized ids of the text.\n",
      " |  \n",
      " |  encode_plus(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a sequence or a pair of sequences.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This method is deprecated, ``__call__`` should be used instead.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]` (the latter only for not-fast tokenizers)):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      " |              ``convert_tokens_to_ids`` method).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n",
      " |              string using the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      " |              ``convert_tokens_to_ids`` method).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
      " |              will skip the pre-tokenization step. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Wheter or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print informations and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 0 specifying added special tokens and 1 specifying\n",
      " |            regual sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  pad(self, encoded_inputs: Union[transformers.tokenization_utils_base.BatchEncoding, List[transformers.tokenization_utils_base.BatchEncoding], Dict[str, List[int]], Dict[str, List[List[int]]], List[Dict[str, List[int]]]], padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = True, max_length: Union[int, NoneType] = None, pad_to_multiple_of: Union[int, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, verbose: bool = True) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n",
      " |      in the batch.\n",
      " |      \n",
      " |      Padding side (left/right) padding token ids are defined at the tokenizer level\n",
      " |      (with ``self.padding_side``, ``self.pad_token_id`` and ``self.pad_token_type_id``)\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          If the ``encoded_inputs`` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n",
      " |          result will use the same type unless you provide a different tensor type with ``return_tensors``. In the\n",
      " |          case of PyTorch tensors, you will lose the specific device of your tensors however.\n",
      " |      \n",
      " |      Args:\n",
      " |          encoded_inputs (:class:`~transformers.BatchEncoding`, list of :class:`~transformers.BatchEncoding`, :obj:`Dict[str, List[int]]`, :obj:`Dict[str, List[List[int]]` or :obj:`List[Dict[str, List[int]]]`):\n",
      " |              Tokenized inputs. Can represent one input (:class:`~transformers.BatchEncoding` or\n",
      " |              :obj:`Dict[str, List[int]]`) or a batch of tokenized inputs (list of\n",
      " |              :class:`~transformers.BatchEncoding`, `Dict[str, List[List[int]]]` or `List[Dict[str, List[int]]]`) so\n",
      " |              you can use this method during preprocessing as well as in a PyTorch Dataloader collate function.\n",
      " |      \n",
      " |              Instead of :obj:`List[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),\n",
      " |              see the note above for the return type.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |               Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
      " |               index) among:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Maximum length of the returned list and optionally padding length (see above).\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value.\n",
      " |      \n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              >= 7.5 (Volta).\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print informations and warnings.\n",
      " |  \n",
      " |  prepare_for_model(self, ids: List[int], pair_ids: Union[List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, prepend_batch_axis: bool = False, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model.\n",
      " |      It adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n",
      " |      manages a moving window (with user defined stride) for overflowing tokens\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the\n",
      " |              ``tokenize`` and ``convert_tokens_to_ids`` methods.\n",
      " |          pair_ids (:obj:`List[int]`, `optional`):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the\n",
      " |              ``tokenize`` and ``convert_tokens_to_ids`` methods.\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
      " |              will skip the pre-tokenization step. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Wheter or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print informations and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 0 specifying added special tokens and 1 specifying\n",
      " |            regual sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: str) -> Tuple[str]\n",
      " |      Save the tokenizer vocabulary files together with:\n",
      " |      \n",
      " |          - added tokens,\n",
      " |          - special tokens to class attributes mapping,\n",
      " |          - tokenizer instantiation positional and keywords inputs (e.g. do_lower_case for Bert).\n",
      " |      \n",
      " |      This method make sure the full tokenizer can then be re-loaded using the\n",
      " |      :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained` class method.\n",
      " |      \n",
      " |      .. Warning::\n",
      " |         This won't save modifications you may have applied to the tokenizer after the instantiation (for instance,\n",
      " |         modifying :obj:`tokenizer.do_lower_case` after creation).\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (:obj:`str`): The path to adirectory where the tokenizer will be saved.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple of :obj:`str`: The files saved.\n",
      " |  \n",
      " |  truncate_sequences(self, ids: List[int], pair_ids: Union[List[int], NoneType] = None, num_tokens_to_remove: int = 0, truncation_strategy: Union[str, transformers.tokenization_utils_base.TruncationStrategy] = 'longest_first', stride: int = 0) -> Tuple[List[int], List[int], List[int]]\n",
      " |      Truncates a sequence pair in-place following the strategy.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the\n",
      " |              ``tokenize`` and ``convert_tokens_to_ids`` methods.\n",
      " |          pair_ids (:obj:`List[int]`, `optional`):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the\n",
      " |              ``tokenize`` and ``convert_tokens_to_ids`` methods.\n",
      " |          num_tokens_to_remove (:obj:`int`, `optional`, defaults to 0):\n",
      " |              Number of tokens to remove using the truncation strategy.\n",
      " |          truncation (:obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              The strategy to follow for truncation. Can be:\n",
      " |      \n",
      " |              * :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a positive number, the overflowing tokens returned will contain some tokens\n",
      " |              from the main sequence returned. The value of this argument defines the number of additional tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple[List[int], List[int], List[int]]`:\n",
      " |          The truncated ``ids``, the truncated ``pair_ids`` and the list of overflowing tokens.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  from_pretrained(*inputs, **kwargs) from builtins.type\n",
      " |      Instantiate a :class:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase` (or a derived class) from\n",
      " |      a predefined tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          pretrained_model_name_or_path (:obj:`str`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |              - A string with the `shortcut name` of a predefined tokenizer to load from cache or download, e.g.,\n",
      " |                ``bert-base-uncased``.\n",
      " |              - A string with the `identifier name` of a predefined tokenizer that was user-uploaded to our S3, e.g.,\n",
      " |                ``dbmdz/bert-base-german-cased``.\n",
      " |              - A path to a `directory` containing vocabulary files required by the tokenizer, for instance saved\n",
      " |                using the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`\n",
      " |                method, e.g., ``./my_model_directory/``.\n",
      " |              - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary\n",
      " |                file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,\n",
      " |                ``./my_model_directory/vocab.txt``.\n",
      " |          cache_dir (:obj:`str`, `optional`):\n",
      " |              Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n",
      " |              exist.\n",
      " |          resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to delete incompletely received files. Attempt to resume the download if such a file\n",
      " |              exists.\n",
      " |          proxies (:obj:`Dict[str, str], `optional`):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g.,\n",
      " |              :obj:`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each\n",
      " |              request.\n",
      " |          inputs (additional positional arguments, `optional`):\n",
      " |              Will be passed along to the Tokenizer ``__init__`` method.\n",
      " |          kwargs (additional keyword arguments, `optional`):\n",
      " |              Will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like\n",
      " |              ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``,\n",
      " |              ``mask_token``, ``additional_special_tokens``. See parameters in the ``__init__`` for more details.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # We can't instantiate directly the base class `PreTrainedTokenizerBase` so let's show our examples on a derived class: BertTokenizer\n",
      " |          # Download vocabulary from S3 and cache.\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          # Download vocabulary from S3 (user-uploaded) and cache.\n",
      " |          tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
      " |      \n",
      " |          # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n",
      " |          tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n",
      " |      \n",
      " |          # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
      " |          tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n",
      " |      \n",
      " |          # You can link tokens to special vocabulary when instantiating\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n",
      " |          # You should be sure '<unk>' is in the vocabulary when doing that.\n",
      " |          # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
      " |          assert tokenizer.unk_token == '<unk>'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  clean_up_tokenization(out_string: str) -> str\n",
      " |      Clean up a list of simple English tokenization artifacts like spaces before punctuations and abreviated forms.\n",
      " |      \n",
      " |      Args:\n",
      " |          out_string (:obj:`str`): The text to clean up.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The cleaned-up string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  max_len\n",
      " |      :obj:`int`: **Deprecated** Kept here for backward compatibility. Now renamed to :obj:`model_max_length` to\n",
      " |      avoid ambiguity.\n",
      " |  \n",
      " |  max_len_sentences_pair\n",
      " |      :obj:`int`: The maximum combined length of a pair of sentences that can be fed to the model.\n",
      " |  \n",
      " |  max_len_single_sentence\n",
      " |      :obj:`int`: The maximum length of a sentence that can be fed to the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  model_input_names = ['token_type_ids', 'attention_mask']\n",
      " |  \n",
      " |  padding_side = 'right'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  add_special_tokens(self, special_tokens_dict: Dict[str, Union[str, tokenizers.AddedToken]]) -> int\n",
      " |      Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If\n",
      " |      special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the\n",
      " |      current vocabulary).\n",
      " |      \n",
      " |      Using : obj:`add_special_tokens` will ensure your special tokens can be used in several ways:\n",
      " |      \n",
      " |      - Special tokens are carefully handled by the tokenizer (they are never split).\n",
      " |      - You can easily refer to special tokens using tokenizer class attributes like :obj:`tokenizer.cls_token`. This\n",
      " |        makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
      " |      \n",
      " |      When possible, special tokens are already registered for provided pretrained models (for instance\n",
      " |      :class:`~transformers.BertTokenizer` :obj:`cls_token` is already registered to be :obj`'[CLS]'` and XLM's one\n",
      " |      is also registered to be :obj:`'</s>'`).\n",
      " |      \n",
      " |      Args:\n",
      " |          special_tokens_dict (dictionary `str` to `str` or :obj:`tokenizers.AddedToken`):\n",
      " |              Keys should be in the list of predefined special attributes: [``bos_token``, ``eos_token``,\n",
      " |              ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n",
      " |              ``additional_special_tokens``].\n",
      " |      \n",
      " |              Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer\n",
      " |              assign the index of the ``unk_token`` to them).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Let's see how to add a new classification token to GPT-2\n",
      " |          tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
      " |          model = GPT2Model.from_pretrained('gpt2')\n",
      " |      \n",
      " |          special_tokens_dict = {'cls_token': '<CLS>'}\n",
      " |      \n",
      " |          num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
      " |          print('We have added', num_added_toks, 'tokens')\n",
      " |          # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |          model.resize_token_embeddings(len(tokenizer))\n",
      " |      \n",
      " |          assert tokenizer.cls_token == '<CLS>'\n",
      " |  \n",
      " |  add_tokens(self, new_tokens: Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]], special_tokens: bool = False) -> int\n",
      " |      Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n",
      " |      it with indices starting from length of the current vocabulary.\n",
      " |      \n",
      " |      Args:\n",
      " |          new_tokens (:obj:`str`, :obj:`tokenizers.AddedToken` or a list of `str` or :obj:`tokenizers.AddedToken`):\n",
      " |              Tokens are only added if they are not already in the vocabulary. :obj:`tokenizers.AddedToken` wraps a\n",
      " |              string token to let you personalize its behavior: whether this token should only match against a single\n",
      " |              word, whether this token should strip all potential whitespaces on the left side, whether this token\n",
      " |              should strip all potential whitespaces on the right side, etc.\n",
      " |          special_token (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Can be used to specify if the token is a special token. This mostly change the normalization behavior\n",
      " |              (special tokens like CLS or [MASK] are usually not lower-cased for instance).\n",
      " |      \n",
      " |              See details for :obj:`tokenizers.AddedToken` in HuggingFace tokenizers library.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
      " |          tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
      " |          model = BertModel.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])\n",
      " |          print('We have added', num_added_toks, 'tokens')\n",
      " |           # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |          model.resize_token_embeddings(len(tokenizer))\n",
      " |  \n",
      " |  sanitize_special_tokens(self) -> int\n",
      " |      Make sure that all the special tokens attributes of the tokenizer (:obj:`tokenizer.mask_token`,\n",
      " |      :obj:`tokenizer.cls_token`, etc.) are in the vocabulary.\n",
      " |      \n",
      " |      Add the missing ones to the vocabulary if needed.\n",
      " |      \n",
      " |      Return:\n",
      " |          :obj:`int`: The number of tokens added in the vocaulary during the operation.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  additional_special_tokens\n",
      " |      :obj:`List[str]`: All the additional special tokens you may want to use. Log an error if used while not having\n",
      " |      been set.\n",
      " |  \n",
      " |  additional_special_tokens_ids\n",
      " |      :obj:`List[int]`: Ids of all the additional special tokens in the vocabulary.\n",
      " |      Log an error if used while not having been set.\n",
      " |  \n",
      " |  all_special_ids\n",
      " |      :obj:`List[int]`: List the ids of the special tokens(:obj:`'<unk>'`, :obj:`'<cls>'`, etc.) mapped to class\n",
      " |      attributes.\n",
      " |  \n",
      " |  all_special_tokens\n",
      " |      :obj:`List[str]`: All the special tokens (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.) mapped to class attributes.\n",
      " |      \n",
      " |      Convert tokens of :obj:`tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  all_special_tokens_extended\n",
      " |      :obj:`List[Union[str, tokenizers.AddedToken]]`: All the special tokens (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.)\n",
      " |      mapped to class attributes.\n",
      " |      \n",
      " |      Don't convert tokens of :obj:`tokenizers.AddedToken` type to string so they can be used to control more finely\n",
      " |      how special tokens are tokenized.\n",
      " |  \n",
      " |  bos_token\n",
      " |      :obj:`str`: Beginning of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  bos_token_id\n",
      " |      :obj:`Optional[int]`: Id of the beginning of sentence token in the vocabulary. Returns :obj:`None` if the token\n",
      " |      has not been set.\n",
      " |  \n",
      " |  cls_token\n",
      " |      :obj:`str`: Classification token, to extract a summary of an input sequence leveraging self-attention along\n",
      " |      the full depth of the model. Log an error if used while not having been set.\n",
      " |  \n",
      " |  cls_token_id\n",
      " |      :obj:`Optional[int]`: Id of the classification token in the vocabulary, to extract a summary of an input\n",
      " |      sequence leveraging self-attention along the full depth of the model.\n",
      " |      \n",
      " |      Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  eos_token\n",
      " |      :obj:`str`: End of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  eos_token_id\n",
      " |      :obj:`Optional[int]`: Id of the end of sentence token in the vocabulary. Returns :obj:`None` if the token has\n",
      " |      not been set.\n",
      " |  \n",
      " |  mask_token\n",
      " |      :obj:`str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while\n",
      " |      not having been set.\n",
      " |  \n",
      " |  mask_token_id\n",
      " |      :obj:`Optional[int]`: Id of the mask token in the vocabulary, used when training a model with masked-language\n",
      " |      modeling. Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  pad_token\n",
      " |      :obj:`str`: Padding token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  pad_token_id\n",
      " |      :obj:`Optional[int]`: Id of the padding token in the vocabulary. Returns :obj:`None` if the token has not been\n",
      " |      set.\n",
      " |  \n",
      " |  pad_token_type_id\n",
      " |      :obj:`int`: Id of the padding token type in the vocabulary.\n",
      " |  \n",
      " |  sep_token\n",
      " |      :obj:`str`: Separation token, to separate context and query in an input sequence.\n",
      " |      Log an error if used while not having been set.\n",
      " |  \n",
      " |  sep_token_id\n",
      " |      :obj:`Optional[int]`: Id of the separation token in the vocabulary, to separate context and query in an input\n",
      " |      sequence. Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  special_tokens_map\n",
      " |      :obj:`Dict[str, Union[str, List[str]]]`: A dictionary mapping special token class attributes\n",
      " |      (:obj:`cls_token`, :obj:`unk_token`, etc.) to their values (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.).\n",
      " |      \n",
      " |      Convert potential tokens of :obj:`tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  special_tokens_map_extended\n",
      " |      :obj:`Dict[str, Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]]]`: A dictionary\n",
      " |      mapping special token class attributes (:obj:`cls_token`, :obj:`unk_token`, etc.) to their values\n",
      " |      (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.).\n",
      " |      \n",
      " |      Don't convert tokens of :obj:`tokenizers.AddedToken` type to string so they can be used to control more finely\n",
      " |      how special tokens are tokenized.\n",
      " |  \n",
      " |  unk_token\n",
      " |      :obj:`str`: Unknown token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  unk_token_id\n",
      " |      :obj:`Optional[int]`: Id of the unknown token in the vocabulary. Returns :obj:`None` if the token has not been\n",
      " |      set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  SPECIAL_TOKENS_ATTRIBUTES = ['bos_token', 'eos_token', 'unk_token', 's...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Internal: /Users/travis/build/google/sentencepiece/src/sentencepiece_processor.cc(818) [model_proto->ParseFromArray(serialized.data(), serialized.size())] ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a1f6936d1f17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vocab.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/transformers/tokenization_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sentencepiece.py\u001b[0m in \u001b[0;36mLoad\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    365\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromSerializedProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sentencepiece.py\u001b[0m in \u001b[0;36mLoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     def Init(self,\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Internal: /Users/travis/build/google/sentencepiece/src/sentencepiece_processor.cc(818) [model_proto->ParseFromArray(serialized.data(), serialized.size())] "
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer('vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class T5Tokenizer in module transformers.tokenization_t5:\n",
      "\n",
      "class T5Tokenizer(transformers.tokenization_utils.PreTrainedTokenizer)\n",
      " |  T5Tokenizer(vocab_file, eos_token='</s>', unk_token='<unk>', pad_token='<pad>', extra_ids=100, additional_special_tokens=None, **kwargs)\n",
      " |  \n",
      " |  Construct a T5 tokenizer. Based on `SentencePiece <https://github.com/google/sentencepiece>`__.\n",
      " |  \n",
      " |  This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the main methods.\n",
      " |  Users should refer to this superclass for more information regarding those methods.\n",
      " |  \n",
      " |  Args:\n",
      " |      vocab_file (:obj:`str`):\n",
      " |          `SentencePiece <https://github.com/google/sentencepiece>`__ file (generally has a `.spm` extension) that\n",
      " |          contains the vocabulary necessary to instantiate a tokenizer.\n",
      " |      eos_token (:obj:`str`, `optional`, defaults to :obj:`\"</s>\"`):\n",
      " |          The end of sequence token.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |              When building a sequence using special tokens, this is not the token that is used for the end\n",
      " |              of sequence. The token used is the :obj:`sep_token`.\n",
      " |      unk_token (:obj:`str`, `optional`, defaults to :obj:`\"<unk>\"`):\n",
      " |          The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
      " |          token instead.\n",
      " |      pad_token (:obj:`str`, `optional`, defaults to :obj:`\"<pad>\"`):\n",
      " |          The token used for padding, for example when batching sequences of different lengths.\n",
      " |      extra_ids (:obj:`int`, `optional`, defaults to 100):\n",
      " |          Add a number of extra ids added to the end of the vocabulary for use as sentinels.\n",
      " |          These tokens are accessible as \"<extra_id_{%d}>\" where \"{%d}\" is a number between 0 and extra_ids-1.\n",
      " |          Extra tokens are indexed from the end of the vocabulary up to beginnning (\"<extra_id_0>\" is the last token\n",
      " |          in the vocabulary like in T5 preprocessing see `here\n",
      " |          <https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117>`__).\n",
      " |      additional_special_tokens (:obj:`List[str]`, `optional`):\n",
      " |          Additional special tokens used by the tokenizer.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      T5Tokenizer\n",
      " |      transformers.tokenization_utils.PreTrainedTokenizer\n",
      " |      transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
      " |      transformers.tokenization_utils_base.SpecialTokensMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self, vocab_file, eos_token='</s>', unk_token='<unk>', pad_token='<pad>', extra_ids=100, additional_special_tokens=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __setstate__(self, d)\n",
      " |  \n",
      " |  build_inputs_with_special_tokens(self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) -> List[int]\n",
      " |      Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
      " |      by concatenating and adding special tokens.\n",
      " |      A sequence has the following format:\n",
      " |      \n",
      " |      - single sequence: ``X </s>``\n",
      " |      - pair of sequences: ``A </s> B </s>``\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`):\n",
      " |              List of IDs to which the special tokens will be added.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`):\n",
      " |              Optional second list of IDs for sequence pairs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`: List of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
      " |  \n",
      " |  convert_tokens_to_string(self, tokens)\n",
      " |      Converts a sequence of tokens (string) in a single string.\n",
      " |  \n",
      " |  get_special_tokens_mask(self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None, already_has_special_tokens: bool = False) -> List[int]\n",
      " |      Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n",
      " |      special tokens using the tokenizer ``prepare_for_model`` method.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`):\n",
      " |              List of IDs.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`):\n",
      " |              Optional second list of IDs for sequence pairs.\n",
      " |          already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the token list is already formatted with special tokens for the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
      " |  \n",
      " |  get_vocab(self)\n",
      " |      Returns the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      :obj:`tokenizer.get_vocab()[token]` is equivalent to :obj:`tokenizer.convert_tokens_to_ids(token)` when\n",
      " |      :obj:`token` is in the vocab.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Dict[str, int]`: The vocabulary.\n",
      " |  \n",
      " |  prepare_seq2seq_batch(self, src_texts: List[str], tgt_texts: Union[List[str], NoneType] = None, max_length: Union[int, NoneType] = None, max_target_length: Union[int, NoneType] = None, padding: str = 'longest', return_tensors: str = None, truncation: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepare model inputs for translation. For best performance, translate one sentence at a time.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          src_texts (:obj:`List[str]`):\n",
      " |              List of documents to summarize or source language texts.\n",
      " |          tgt_texts (:obj:`list`, `optional`):\n",
      " |              List of summaries or target language texts.\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length for encoder inputs (documents to summarize or source language texts)\n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          max_target_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length of decoder inputs (target language texts or summaries)\n",
      " |              If left unset or set to :obj:`None`, this will use the max_length value.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`, defaults to \"pt\"):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`True`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          **kwargs:\n",
      " |              Additional keyword arguments passed along to :obj:`self.__call__`.\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to the encoder.\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.\n",
      " |          - **decoder_input_ids** -- List of token ids to be fed to the decoder.\n",
      " |          - **decoder_attention_mask** -- List of indices specifying which tokens should be attended to by the decoder.\n",
      " |              This does not include causal mask, which is built by the model.\n",
      " |      \n",
      " |          The full set of keys ``[input_ids, attention_mask, decoder_input_ids,  decoder_attention_mask]``,\n",
      " |          will only be returned if tgt_texts is passed. Otherwise, input_ids, attention_mask will be the only keys.\n",
      " |  \n",
      " |  save_vocabulary(self, save_directory)\n",
      " |      Save the sentencepiece vocabulary (copy original file) and special tokens file to a directory.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (:obj:`str`):\n",
      " |              The directory in which to save the vocabulary.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple(str)`: Paths to the files saved.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  vocab_size\n",
      " |      :obj:`int`: Size of the base vocabulary (without the added tokens).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  max_model_input_sizes = {'t5-11b': 512, 't5-3b': 512, 't5-base': 512, ...\n",
      " |  \n",
      " |  model_input_names = ['attention_mask']\n",
      " |  \n",
      " |  pretrained_vocab_files_map = {'vocab_file': {'t5-11b': 'https://s3.ama...\n",
      " |  \n",
      " |  vocab_files_names = {'vocab_file': 'spiece.model'}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Size of the full vocabulary with the added tokens.\n",
      " |  \n",
      " |  convert_ids_to_tokens(self, ids: Union[int, List[int]], skip_special_tokens: bool = False) -> Union[str, List[str]]\n",
      " |      Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary\n",
      " |      and added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`int` or :obj:`List[int]`):\n",
      " |              The token id (or token ids) to convert to tokens.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str` or :obj:`List[str]`: The decoded token(s).\n",
      " |  \n",
      " |  convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]\n",
      " |      Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the\n",
      " |      vocabulary.\n",
      " |      \n",
      " |      Args:\n",
      " |          token (:obj:`str` or :obj:`List[str]`): One or several token(s) to convert to token id(s).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int` or :obj:`List[int]`: The token id or list of token ids.\n",
      " |  \n",
      " |  decode(self, token_ids: List[int], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = True) -> str\n",
      " |      Converts a sequence of ids in a string, using the tokenizer and vocabulary\n",
      " |      with options to remove special tokens and clean up tokenization spaces.\n",
      " |      \n",
      " |      Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids (:obj:`List[int]`):\n",
      " |              List of tokenized input ids. Can be obtained using the ``__call__`` method.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to clean up the tokenization spaces.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The decoded sentence.\n",
      " |  \n",
      " |  get_added_vocab(self) -> Dict[str, int]\n",
      " |      Returns the added tokens in the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Dict[str, int]`: The added tokens.\n",
      " |  \n",
      " |  num_special_tokens_to_add(self, pair: bool = False) -> int\n",
      " |      Returns the number of added tokens when encoding a sequence with special tokens.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not\n",
      " |          put this inside your training loop.\n",
      " |      \n",
      " |      Args:\n",
      " |          pair (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether the number of added tokens should be computed in the case of a sequence pair or a single\n",
      " |              sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of special tokens added to sequences.\n",
      " |  \n",
      " |  prepare_for_tokenization(self, text: str, is_split_into_words: bool = False, **kwargs) -> Tuple[str, Dict[str, Any]]\n",
      " |      Performs any necessary transformations before tokenization.\n",
      " |      \n",
      " |      This method should pop the arguments from kwargs and return the remaining :obj:`kwargs` as well.\n",
      " |      We test the :obj:`kwargs` at the end of the encoding process to be sure all the arguments have been used.\n",
      " |      \n",
      " |      Args:\n",
      " |          test (:obj:`str`):\n",
      " |              The text to prepare.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the text has been pretokenized.\n",
      " |          kwargs:\n",
      " |              Keyword arguments to use for the tokenization.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple[str, Dict[str, Any]]`: The prepared text and the unused kwargs.\n",
      " |  \n",
      " |  tokenize(self, text: str, **kwargs) -> List[str]\n",
      " |      Converts a string in a sequence of tokens, using the tokenizer.\n",
      " |      \n",
      " |      Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies (BPE/SentencePieces/WordPieces).\n",
      " |      Takes care of added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`):\n",
      " |              The sequence to be encoded.\n",
      " |          **kwargs (additional keyword arguments):\n",
      " |              Passed along to the model-specific ``prepare_for_tokenization`` preprocessing method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[str]`: The list of tokens.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  is_fast\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, List[str], List[List[str]]], text_pair: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      " |      sequences.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):\n",
      " |              The sequence or batch of sequences to be encoded.\n",
      " |              Each sequence can be a string or a list of strings (pretokenized string).\n",
      " |              If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):\n",
      " |              The sequence or batch of sequences to be encoded.\n",
      " |              Each sequence can be a string or a list of strings (pretokenized string).\n",
      " |              If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
      " |              will skip the pre-tokenization step. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Wheter or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print informations and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 0 specifying added special tokens and 1 specifying\n",
      " |            regual sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  batch_decode(self, sequences: List[List[int]], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = True) -> List[str]\n",
      " |      Convert a list of lists of token ids into a list of strings by calling decode.\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences (:obj:`List[List[int]]`):\n",
      " |              List of tokenized input ids. Can be obtained using the ``__call__`` method.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to clean up the tokenization spaces.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[str]`: The list of decoded sentences.\n",
      " |  \n",
      " |  batch_encode_plus(self, batch_text_or_text_pairs: Union[List[str], List[Tuple[str, str]], List[List[str]], List[Tuple[List[str], List[str]]], List[List[int]], List[Tuple[List[int], List[int]]]], add_special_tokens: bool = True, padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This method is deprecated, ``__call__`` should be used instead.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_text_or_text_pairs (:obj:`List[str]`, :obj:`List[Tuple[str, str]]`, :obj:`List[List[str]]`, :obj:`List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also :obj:`List[List[int]]`, :obj:`List[Tuple[List[int], List[int]]]`):\n",
      " |              Batch of sequences or pair of sequences to be encoded.\n",
      " |              This can be a list of string/string-sequences/int-sequences or a list of pair of\n",
      " |              string/string-sequences/int-sequence (see details in ``encode_plus``).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
      " |              will skip the pre-tokenization step. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Wheter or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print informations and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 0 specifying added special tokens and 1 specifying\n",
      " |            regual sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  create_token_type_ids_from_sequences(self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) -> List[int]\n",
      " |      Create the token type IDs corresponding to the sequences passed.\n",
      " |      `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |      Should be overriden in a subclass if the model has a special way of building those.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`): The first tokenized sequence.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`): The second tokenized sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`: The token type ids.\n",
      " |  \n",
      " |  encode(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, **kwargs) -> List[int]\n",
      " |      Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
      " |      \n",
      " |      Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      " |              ``convert_tokens_to_ids`` method).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n",
      " |              string using the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      " |              ``convert_tokens_to_ids`` method).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
      " |              will skip the pre-tokenization step. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          **kwargs: Passed along to the `.tokenize()` method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`, :obj:`torch.Tensor`, :obj:`tf.Tensor` or :obj:`np.ndarray`:\n",
      " |          The tokenized ids of the text.\n",
      " |  \n",
      " |  encode_plus(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a sequence or a pair of sequences.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This method is deprecated, ``__call__`` should be used instead.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]` (the latter only for not-fast tokenizers)):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      " |              ``convert_tokens_to_ids`` method).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n",
      " |              string using the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      " |              ``convert_tokens_to_ids`` method).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
      " |              will skip the pre-tokenization step. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Wheter or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print informations and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 0 specifying added special tokens and 1 specifying\n",
      " |            regual sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  pad(self, encoded_inputs: Union[transformers.tokenization_utils_base.BatchEncoding, List[transformers.tokenization_utils_base.BatchEncoding], Dict[str, List[int]], Dict[str, List[List[int]]], List[Dict[str, List[int]]]], padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = True, max_length: Union[int, NoneType] = None, pad_to_multiple_of: Union[int, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, verbose: bool = True) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n",
      " |      in the batch.\n",
      " |      \n",
      " |      Padding side (left/right) padding token ids are defined at the tokenizer level\n",
      " |      (with ``self.padding_side``, ``self.pad_token_id`` and ``self.pad_token_type_id``)\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          If the ``encoded_inputs`` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n",
      " |          result will use the same type unless you provide a different tensor type with ``return_tensors``. In the\n",
      " |          case of PyTorch tensors, you will lose the specific device of your tensors however.\n",
      " |      \n",
      " |      Args:\n",
      " |          encoded_inputs (:class:`~transformers.BatchEncoding`, list of :class:`~transformers.BatchEncoding`, :obj:`Dict[str, List[int]]`, :obj:`Dict[str, List[List[int]]` or :obj:`List[Dict[str, List[int]]]`):\n",
      " |              Tokenized inputs. Can represent one input (:class:`~transformers.BatchEncoding` or\n",
      " |              :obj:`Dict[str, List[int]]`) or a batch of tokenized inputs (list of\n",
      " |              :class:`~transformers.BatchEncoding`, `Dict[str, List[List[int]]]` or `List[Dict[str, List[int]]]`) so\n",
      " |              you can use this method during preprocessing as well as in a PyTorch Dataloader collate function.\n",
      " |      \n",
      " |              Instead of :obj:`List[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),\n",
      " |              see the note above for the return type.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |               Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
      " |               index) among:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Maximum length of the returned list and optionally padding length (see above).\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value.\n",
      " |      \n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              >= 7.5 (Volta).\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print informations and warnings.\n",
      " |  \n",
      " |  prepare_for_model(self, ids: List[int], pair_ids: Union[List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.tokenization_utils_base.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.tokenization_utils_base.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, prepend_batch_axis: bool = False, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model.\n",
      " |      It adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n",
      " |      manages a moving window (with user defined stride) for overflowing tokens\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the\n",
      " |              ``tokenize`` and ``convert_tokens_to_ids`` methods.\n",
      " |          pair_ids (:obj:`List[int]`, `optional`):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the\n",
      " |              ``tokenize`` and ``convert_tokens_to_ids`` methods.\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer\n",
      " |              will skip the pre-tokenization step. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Wheter or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print informations and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 0 specifying added special tokens and 1 specifying\n",
      " |            regual sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: str) -> Tuple[str]\n",
      " |      Save the tokenizer vocabulary files together with:\n",
      " |      \n",
      " |          - added tokens,\n",
      " |          - special tokens to class attributes mapping,\n",
      " |          - tokenizer instantiation positional and keywords inputs (e.g. do_lower_case for Bert).\n",
      " |      \n",
      " |      This method make sure the full tokenizer can then be re-loaded using the\n",
      " |      :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained` class method.\n",
      " |      \n",
      " |      .. Warning::\n",
      " |         This won't save modifications you may have applied to the tokenizer after the instantiation (for instance,\n",
      " |         modifying :obj:`tokenizer.do_lower_case` after creation).\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (:obj:`str`): The path to adirectory where the tokenizer will be saved.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple of :obj:`str`: The files saved.\n",
      " |  \n",
      " |  truncate_sequences(self, ids: List[int], pair_ids: Union[List[int], NoneType] = None, num_tokens_to_remove: int = 0, truncation_strategy: Union[str, transformers.tokenization_utils_base.TruncationStrategy] = 'longest_first', stride: int = 0) -> Tuple[List[int], List[int], List[int]]\n",
      " |      Truncates a sequence pair in-place following the strategy.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the\n",
      " |              ``tokenize`` and ``convert_tokens_to_ids`` methods.\n",
      " |          pair_ids (:obj:`List[int]`, `optional`):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the\n",
      " |              ``tokenize`` and ``convert_tokens_to_ids`` methods.\n",
      " |          num_tokens_to_remove (:obj:`int`, `optional`, defaults to 0):\n",
      " |              Number of tokens to remove using the truncation strategy.\n",
      " |          truncation (:obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              The strategy to follow for truncation. Can be:\n",
      " |      \n",
      " |              * :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a positive number, the overflowing tokens returned will contain some tokens\n",
      " |              from the main sequence returned. The value of this argument defines the number of additional tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple[List[int], List[int], List[int]]`:\n",
      " |          The truncated ``ids``, the truncated ``pair_ids`` and the list of overflowing tokens.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  from_pretrained(*inputs, **kwargs) from builtins.type\n",
      " |      Instantiate a :class:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase` (or a derived class) from\n",
      " |      a predefined tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          pretrained_model_name_or_path (:obj:`str`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |              - A string with the `shortcut name` of a predefined tokenizer to load from cache or download, e.g.,\n",
      " |                ``bert-base-uncased``.\n",
      " |              - A string with the `identifier name` of a predefined tokenizer that was user-uploaded to our S3, e.g.,\n",
      " |                ``dbmdz/bert-base-german-cased``.\n",
      " |              - A path to a `directory` containing vocabulary files required by the tokenizer, for instance saved\n",
      " |                using the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`\n",
      " |                method, e.g., ``./my_model_directory/``.\n",
      " |              - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary\n",
      " |                file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,\n",
      " |                ``./my_model_directory/vocab.txt``.\n",
      " |          cache_dir (:obj:`str`, `optional`):\n",
      " |              Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n",
      " |              exist.\n",
      " |          resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to delete incompletely received files. Attempt to resume the download if such a file\n",
      " |              exists.\n",
      " |          proxies (:obj:`Dict[str, str], `optional`):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g.,\n",
      " |              :obj:`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each\n",
      " |              request.\n",
      " |          inputs (additional positional arguments, `optional`):\n",
      " |              Will be passed along to the Tokenizer ``__init__`` method.\n",
      " |          kwargs (additional keyword arguments, `optional`):\n",
      " |              Will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like\n",
      " |              ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``,\n",
      " |              ``mask_token``, ``additional_special_tokens``. See parameters in the ``__init__`` for more details.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # We can't instantiate directly the base class `PreTrainedTokenizerBase` so let's show our examples on a derived class: BertTokenizer\n",
      " |          # Download vocabulary from S3 and cache.\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          # Download vocabulary from S3 (user-uploaded) and cache.\n",
      " |          tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
      " |      \n",
      " |          # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n",
      " |          tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n",
      " |      \n",
      " |          # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
      " |          tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n",
      " |      \n",
      " |          # You can link tokens to special vocabulary when instantiating\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n",
      " |          # You should be sure '<unk>' is in the vocabulary when doing that.\n",
      " |          # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
      " |          assert tokenizer.unk_token == '<unk>'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  clean_up_tokenization(out_string: str) -> str\n",
      " |      Clean up a list of simple English tokenization artifacts like spaces before punctuations and abreviated forms.\n",
      " |      \n",
      " |      Args:\n",
      " |          out_string (:obj:`str`): The text to clean up.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The cleaned-up string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  max_len\n",
      " |      :obj:`int`: **Deprecated** Kept here for backward compatibility. Now renamed to :obj:`model_max_length` to\n",
      " |      avoid ambiguity.\n",
      " |  \n",
      " |  max_len_sentences_pair\n",
      " |      :obj:`int`: The maximum combined length of a pair of sentences that can be fed to the model.\n",
      " |  \n",
      " |  max_len_single_sentence\n",
      " |      :obj:`int`: The maximum length of a sentence that can be fed to the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  __annotations__ = {'max_model_input_sizes': typing.Dict[str, typing.Un...\n",
      " |  \n",
      " |  padding_side = 'right'\n",
      " |  \n",
      " |  pretrained_init_configuration = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  add_special_tokens(self, special_tokens_dict: Dict[str, Union[str, tokenizers.AddedToken]]) -> int\n",
      " |      Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If\n",
      " |      special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the\n",
      " |      current vocabulary).\n",
      " |      \n",
      " |      Using : obj:`add_special_tokens` will ensure your special tokens can be used in several ways:\n",
      " |      \n",
      " |      - Special tokens are carefully handled by the tokenizer (they are never split).\n",
      " |      - You can easily refer to special tokens using tokenizer class attributes like :obj:`tokenizer.cls_token`. This\n",
      " |        makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
      " |      \n",
      " |      When possible, special tokens are already registered for provided pretrained models (for instance\n",
      " |      :class:`~transformers.BertTokenizer` :obj:`cls_token` is already registered to be :obj`'[CLS]'` and XLM's one\n",
      " |      is also registered to be :obj:`'</s>'`).\n",
      " |      \n",
      " |      Args:\n",
      " |          special_tokens_dict (dictionary `str` to `str` or :obj:`tokenizers.AddedToken`):\n",
      " |              Keys should be in the list of predefined special attributes: [``bos_token``, ``eos_token``,\n",
      " |              ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n",
      " |              ``additional_special_tokens``].\n",
      " |      \n",
      " |              Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer\n",
      " |              assign the index of the ``unk_token`` to them).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Let's see how to add a new classification token to GPT-2\n",
      " |          tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
      " |          model = GPT2Model.from_pretrained('gpt2')\n",
      " |      \n",
      " |          special_tokens_dict = {'cls_token': '<CLS>'}\n",
      " |      \n",
      " |          num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
      " |          print('We have added', num_added_toks, 'tokens')\n",
      " |          # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |          model.resize_token_embeddings(len(tokenizer))\n",
      " |      \n",
      " |          assert tokenizer.cls_token == '<CLS>'\n",
      " |  \n",
      " |  add_tokens(self, new_tokens: Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]], special_tokens: bool = False) -> int\n",
      " |      Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n",
      " |      it with indices starting from length of the current vocabulary.\n",
      " |      \n",
      " |      Args:\n",
      " |          new_tokens (:obj:`str`, :obj:`tokenizers.AddedToken` or a list of `str` or :obj:`tokenizers.AddedToken`):\n",
      " |              Tokens are only added if they are not already in the vocabulary. :obj:`tokenizers.AddedToken` wraps a\n",
      " |              string token to let you personalize its behavior: whether this token should only match against a single\n",
      " |              word, whether this token should strip all potential whitespaces on the left side, whether this token\n",
      " |              should strip all potential whitespaces on the right side, etc.\n",
      " |          special_token (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Can be used to specify if the token is a special token. This mostly change the normalization behavior\n",
      " |              (special tokens like CLS or [MASK] are usually not lower-cased for instance).\n",
      " |      \n",
      " |              See details for :obj:`tokenizers.AddedToken` in HuggingFace tokenizers library.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
      " |          tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
      " |          model = BertModel.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])\n",
      " |          print('We have added', num_added_toks, 'tokens')\n",
      " |           # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |          model.resize_token_embeddings(len(tokenizer))\n",
      " |  \n",
      " |  sanitize_special_tokens(self) -> int\n",
      " |      Make sure that all the special tokens attributes of the tokenizer (:obj:`tokenizer.mask_token`,\n",
      " |      :obj:`tokenizer.cls_token`, etc.) are in the vocabulary.\n",
      " |      \n",
      " |      Add the missing ones to the vocabulary if needed.\n",
      " |      \n",
      " |      Return:\n",
      " |          :obj:`int`: The number of tokens added in the vocaulary during the operation.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  additional_special_tokens\n",
      " |      :obj:`List[str]`: All the additional special tokens you may want to use. Log an error if used while not having\n",
      " |      been set.\n",
      " |  \n",
      " |  additional_special_tokens_ids\n",
      " |      :obj:`List[int]`: Ids of all the additional special tokens in the vocabulary.\n",
      " |      Log an error if used while not having been set.\n",
      " |  \n",
      " |  all_special_ids\n",
      " |      :obj:`List[int]`: List the ids of the special tokens(:obj:`'<unk>'`, :obj:`'<cls>'`, etc.) mapped to class\n",
      " |      attributes.\n",
      " |  \n",
      " |  all_special_tokens\n",
      " |      :obj:`List[str]`: All the special tokens (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.) mapped to class attributes.\n",
      " |      \n",
      " |      Convert tokens of :obj:`tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  all_special_tokens_extended\n",
      " |      :obj:`List[Union[str, tokenizers.AddedToken]]`: All the special tokens (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.)\n",
      " |      mapped to class attributes.\n",
      " |      \n",
      " |      Don't convert tokens of :obj:`tokenizers.AddedToken` type to string so they can be used to control more finely\n",
      " |      how special tokens are tokenized.\n",
      " |  \n",
      " |  bos_token\n",
      " |      :obj:`str`: Beginning of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  bos_token_id\n",
      " |      :obj:`Optional[int]`: Id of the beginning of sentence token in the vocabulary. Returns :obj:`None` if the token\n",
      " |      has not been set.\n",
      " |  \n",
      " |  cls_token\n",
      " |      :obj:`str`: Classification token, to extract a summary of an input sequence leveraging self-attention along\n",
      " |      the full depth of the model. Log an error if used while not having been set.\n",
      " |  \n",
      " |  cls_token_id\n",
      " |      :obj:`Optional[int]`: Id of the classification token in the vocabulary, to extract a summary of an input\n",
      " |      sequence leveraging self-attention along the full depth of the model.\n",
      " |      \n",
      " |      Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  eos_token\n",
      " |      :obj:`str`: End of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  eos_token_id\n",
      " |      :obj:`Optional[int]`: Id of the end of sentence token in the vocabulary. Returns :obj:`None` if the token has\n",
      " |      not been set.\n",
      " |  \n",
      " |  mask_token\n",
      " |      :obj:`str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while\n",
      " |      not having been set.\n",
      " |  \n",
      " |  mask_token_id\n",
      " |      :obj:`Optional[int]`: Id of the mask token in the vocabulary, used when training a model with masked-language\n",
      " |      modeling. Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  pad_token\n",
      " |      :obj:`str`: Padding token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  pad_token_id\n",
      " |      :obj:`Optional[int]`: Id of the padding token in the vocabulary. Returns :obj:`None` if the token has not been\n",
      " |      set.\n",
      " |  \n",
      " |  pad_token_type_id\n",
      " |      :obj:`int`: Id of the padding token type in the vocabulary.\n",
      " |  \n",
      " |  sep_token\n",
      " |      :obj:`str`: Separation token, to separate context and query in an input sequence.\n",
      " |      Log an error if used while not having been set.\n",
      " |  \n",
      " |  sep_token_id\n",
      " |      :obj:`Optional[int]`: Id of the separation token in the vocabulary, to separate context and query in an input\n",
      " |      sequence. Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  special_tokens_map\n",
      " |      :obj:`Dict[str, Union[str, List[str]]]`: A dictionary mapping special token class attributes\n",
      " |      (:obj:`cls_token`, :obj:`unk_token`, etc.) to their values (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.).\n",
      " |      \n",
      " |      Convert potential tokens of :obj:`tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  special_tokens_map_extended\n",
      " |      :obj:`Dict[str, Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]]]`: A dictionary\n",
      " |      mapping special token class attributes (:obj:`cls_token`, :obj:`unk_token`, etc.) to their values\n",
      " |      (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.).\n",
      " |      \n",
      " |      Don't convert tokens of :obj:`tokenizers.AddedToken` type to string so they can be used to control more finely\n",
      " |      how special tokens are tokenized.\n",
      " |  \n",
      " |  unk_token\n",
      " |      :obj:`str`: Unknown token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  unk_token_id\n",
      " |      :obj:`Optional[int]`: Id of the unknown token in the vocabulary. Returns :obj:`None` if the token has not been\n",
      " |      set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  SPECIAL_TOKENS_ATTRIBUTES = ['bos_token', 'eos_token', 'unk_token', 's...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(T5Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
