{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5-VAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sjTxtGO6gog",
        "colab_type": "text"
      },
      "source": [
        "# T5-VAE\n",
        "\n",
        "Here you can try using a T5-VAE trained on Python state changes.\n",
        "\n",
        "It tries to learn a smooth latent space of Python assignments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEHLFnsR6eOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp -r gs://fras/python_assignments_autoencoder .\n",
        "!git clone https://github.com/Fraser-Greenlee/T5-VAE.git\n",
        "!pip install transformers==3.0.2 wandb tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaxEDaCz71UU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('T5-VAE/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNjWrpJM70F0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e34cdf95-fe52-4974-a6aa-bc17afa0dc3a"
      },
      "source": [
        "import t5_vae\n",
        "\n",
        "def args_list_from_txt(args_txt):\n",
        "    l = [args_line.strip().split(' ') for args_line in args_txt.strip().split('\\n')]\n",
        "    return [item for sublist in l for item in sublist]\n",
        "\n",
        "def load_t5_vae_from_path(t5_vae_path):\n",
        "    with open( os.path.join(t5_vae_path, 'args.txt'), 'r' ) as f:\n",
        "        args_txt = f.read()\n",
        "    args_list = args_list_from_txt(args_txt)\n",
        "    return t5_vae.load_t5_vae_from_args(args_list)\n",
        "\n",
        "model = load_t5_vae_from_path('python_assignments_autoencoder')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt9Rvz168rjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "model = model.to('cuda').eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMNc6-2791on",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "def str_to_input_ids(model, input_str):\n",
        "    return model.pad_input_ids(\n",
        "        torch.tensor(\n",
        "            model.tokenizer.encode(input_str),\n",
        "            device='cuda'\n",
        "        )\n",
        "    )\n",
        "\n",
        "def logits_to_str(model, logits):\n",
        "    return model.tokenizer.decode(torch.topk(logits[0], 1).indices.view(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YdcXVOv97Fo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f2ecf582-cca4-4fbf-f97c-1134d4c5f28f"
      },
      "source": [
        "# recreate the input string\n",
        "INPUT_STR = 'x = 100;'\n",
        "input_ids = str_to_input_ids(model, INPUT_STR)\n",
        "logits = model.greedy_logits(input_ids=input_ids)\n",
        "logits_to_str(model, logits)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'x = 100;'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wFHI54_9q1m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "87a7a861-e140-465b-9f49-fba6f3f64758"
      },
      "source": [
        "# traverse between 2 assignments\n",
        "latent1 = model.get_latent(str_to_input_ids(model, 'x = a - 1;'))\n",
        "latent2 = model.get_latent(str_to_input_ids(model, 'x = a + 10 * 2;'))\n",
        "\n",
        "latent_diff = latent2 - latent1\n",
        "latent_start = latent1\n",
        "\n",
        "for i in range(11):\n",
        "    ratio = i/10\n",
        "    latent = latent_start + latent_diff * ratio\n",
        "    logits = model.greedy_logits(latent=latent)\n",
        "    print(ratio, logits_to_str(model, logits))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 x = a - 1;\n",
            "0.1 x = a - 1;\n",
            "0.2 x = a - 1;\n",
            "0.3 x = a - 1;\n",
            "0.4 x = a + 1;\n",
            "0.5 x = a + 2;\n",
            "0.6 x = a + 2;\n",
            "0.7 x = a + 2 * 2;\n",
            "0.8 x = a + 10 * 2;\n",
            "0.9 x = a + 10 * 2;\n",
            "1.0 x = a + 10 * 2;\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY5YeV1V_aAN",
        "colab_type": "text"
      },
      "source": [
        "Above you can see that all intermediate values valid Python assignments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpazCY86-Zv5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6f3f73f7-7bed-4259-b0ac-18a8d2ef6793"
      },
      "source": [
        "# test recreating a range of inputs\n",
        "input_temp = 'x = a + {0};'\n",
        "for i in range(50)[::4]:\n",
        "    input_str = input_temp.format(i)\n",
        "    input_ids = str_to_input_ids(model, input_str)\n",
        "    logits = model.greedy_logits(input_ids=input_ids)\n",
        "    print(input_str, logits_to_str(model, logits))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x = a + 0; x = a + 0;\n",
            "x = a + 4; x = a + 4;\n",
            "x = a + 8; x = a + 8;\n",
            "x = a + 12; x = a + 12;\n",
            "x = a + 16; x = a + 16;\n",
            "x = a + 20; x = a + count;\n",
            "x = a + 24; x = a + 100;\n",
            "x = a + 28; x = a + mult;\n",
            "x = a + 32; x = a + 32;\n",
            "x = a + 36; x = a + 12;\n",
            "x = a + 40; x = a + height;\n",
            "x = a + 44; x = a + bit;\n",
            "x = a + 48; x = a + bit;\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxMAqx_e_jHM",
        "colab_type": "text"
      },
      "source": [
        "When applied to a range of values you can see the model has holes, this is likely due to the training dataset not providing a dense enough sampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_l4pzxS_EdF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "19c3bb9f-1905-4fb0-d2ea-4f6d5af03868"
      },
      "source": [
        "# sample from random latent values\n",
        "with torch.no_grad():\n",
        "  for _ in range(10):\n",
        "      logits = model.greedy_logits(latent=torch.randn(1, 1000).to('cuda'))\n",
        "      print(logits_to_str(model, logits))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "er = int(h[3] * 0);\n",
            "l.append([False[j] * d);\n",
            "y = '[0 '] = 1;\n",
            "x = int(h[-1] * 0);\n",
            "l.append( = 0 + str(x[0 / 1]);\n",
            "x.append(a[da] * 0);\n",
            "x =''[0 - 1:0];\n",
            "x.append(x.pop(  + 1) ** 0);\n",
            "f = int(h[i].pop() + 1);\n",
            "x = int(h[-1 - 1]);\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLwCmAW9BrW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}